# Knowledge Distillation 

### Goal  
Given a scientific task and a teacher model(large LLM) that is capable at handling language but is not trained on scientific tasks (not a specialist), can we train/use a student model that is very good at this task and learn the language capabilities from the teacher model through knowledge distillation? 