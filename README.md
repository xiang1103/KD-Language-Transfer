# Knowledge Distillation 

### Goal  
Given a scientific task and a teacher model(large LLM) that is capable at handling language but is not trained on scientific tasks (not a specialist), can we train/use a student model that is very good at this task and learn the language capabilities from the teacher model through knowledge distillation? 

### Methodology 
Teacher Model: TBD 
Student Model:TBD    
  
### Steps needed: 
[ ] Read the ACL paper to understand how to use Language Model on scientific tasks, and the different tasks we can potentially use for this project.   
[ ] Literature search: Find similar papers that used     
- Knowledge Distillation to train student model to get language capabilities 
- Specialist model at chemistry/material science/other kinds of scientific knowledge
- Pre-trained models that are successful at doing specific tasks (ex: Chemformer, MolGPT). We might need these as the student model 

[ ] Student Model search   
[ ] Teacher Model search 